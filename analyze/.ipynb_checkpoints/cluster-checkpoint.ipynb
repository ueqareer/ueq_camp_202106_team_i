{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naoki_ito/.pyenv/versions/anaconda3-2020.11/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "/Users/naoki_ito/.pyenv/versions/anaconda3-2020.11/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:938: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    }
   ],
   "source": [
    "#参考: https://qiita.com/toshiyuki_tsutsui/items/b3ac8fd1b300c3404508\n",
    "\n",
    "from collections import defaultdict\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KeyedVectors.load('../model/word2vec_ramen_model.model')\n",
    "\n",
    "max_vocab = 30000 #40000にしても結果は同じだった\n",
    "vocab = list(model.wv.index_to_key)[:max_vocab] #vocab.key()->index_to_key\n",
    "vectors = [model.wv[word] for word in vocab]\n",
    "\n",
    "n_clusters = 6 #クラスター数はこちらで任意の値を定める\n",
    "kmeans_model = KMeans(n_clusters=n_clusters, verbose=0, random_state=42, n_jobs=-1)#n_jobsがもうそろそろ消えるらしい\n",
    "kmeans_model.fit(vectors)\n",
    "\n",
    "cluster_labels = kmeans_model.labels_\n",
    "cluster_to_words = defaultdict(list)\n",
    "for cluster_id, word in zip(cluster_labels, vocab):\n",
    "    cluster_to_words[cluster_id].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['スープ', '醤油', '旨味', '香り', '出汁', '煮干し', '味わい', '風味', '昆布', '魚介', 'なく', 'ストレート', 'バランス', '加水', '見た目', '自家製', 'コク', '印象', '上品', 'しじみ']\n",
      "['チャーシュー', '美味しい', 'これ', 'ワンタン', '感じ', '良い', 'メンマ', 'トッピング', '調理', '食感', '良く', '低温', '具材', '海老', '旨い', '好み', '美味い', 'ネギ', '海苔', 'ロース']\n",
      "['ラーメン', 'こと', 'よう', '中華そば', 'こちら', 'そう', 'ない', 'いい', 'もの', '人気', 'ところ', '今回', 'それ', '一杯', '山椒', '高い', 'つけ麺', '好き', '美味しかっ', 'ここ']\n",
      "['さん', '訪問', '行列', '到着', '待ち', '時間', '開店', '入店', '平日', '10分', '11時', 'ため', '30分', '近く', '11', '並び', '10', '30', '営業', '12時']\n",
      "['そば', '特製', '店内', '食券', 'カウンター', '店主', '注文', '購入', 'メニュー', '券売機', '丁寧', 'らーめん', '提供', 'つけ', '醤油ラーメン', '店員', '着席', '軍鶏', '綺麗', '限定']\n",
      "['食べログ', 'オープン', '徒歩', '名店', '百名', '場所', 'ラーメン店', '中華蕎麦', '東京', '店舗', '製麺', '有名', '2020', '移転', '2020年', '秋葉原', 'ビブグルマン', 'https', '選出', 'ランチ']\n"
     ]
    }
   ],
   "source": [
    "for words in cluster_to_words.values():\n",
    "    print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ラーメンの中身に関するワード</th>\n",
       "      <th>券売機や注文に関するワード</th>\n",
       "      <th>人や接客、内装に関するワード</th>\n",
       "      <th>曜日時間、店舗の地理的なワード</th>\n",
       "      <th>日付、お店の評価、ネット用語に関するワード</th>\n",
       "      <th>その他のワード</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>そば</td>\n",
       "      <td>スープ</td>\n",
       "      <td>チャーシュー</td>\n",
       "      <td>食べログ</td>\n",
       "      <td>ラーメン</td>\n",
       "      <td>さん</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>特製</td>\n",
       "      <td>醤油</td>\n",
       "      <td>美味しい</td>\n",
       "      <td>オープン</td>\n",
       "      <td>こと</td>\n",
       "      <td>訪問</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>店内</td>\n",
       "      <td>旨味</td>\n",
       "      <td>これ</td>\n",
       "      <td>徒歩</td>\n",
       "      <td>よう</td>\n",
       "      <td>行列</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>食券</td>\n",
       "      <td>香り</td>\n",
       "      <td>ワンタン</td>\n",
       "      <td>名店</td>\n",
       "      <td>中華そば</td>\n",
       "      <td>到着</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>カウンター</td>\n",
       "      <td>出汁</td>\n",
       "      <td>感じ</td>\n",
       "      <td>百名</td>\n",
       "      <td>こちら</td>\n",
       "      <td>待ち</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>づくり</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>50%</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>否が応でも</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>渾身</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>映画</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1027 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ラーメンの中身に関するワード 券売機や注文に関するワード 人や接客、内装に関するワード 曜日時間、店舗の地理的なワード  \\\n",
       "0                そば           スープ         チャーシュー            食べログ   \n",
       "1                特製            醤油           美味しい            オープン   \n",
       "2                店内            旨味             これ              徒歩   \n",
       "3                食券            香り           ワンタン              名店   \n",
       "4             カウンター            出汁             感じ              百名   \n",
       "...             ...           ...            ...             ...   \n",
       "1022           None          None           None            None   \n",
       "1023           None          None           None            None   \n",
       "1024           None          None           None            None   \n",
       "1025           None          None           None            None   \n",
       "1026           None          None           None            None   \n",
       "\n",
       "     日付、お店の評価、ネット用語に関するワード その他のワード  \n",
       "0                     ラーメン      さん  \n",
       "1                       こと      訪問  \n",
       "2                       よう      行列  \n",
       "3                     中華そば      到着  \n",
       "4                      こちら      待ち  \n",
       "...                    ...     ...  \n",
       "1022                   づくり    None  \n",
       "1023                   50%    None  \n",
       "1024                 否が応でも    None  \n",
       "1025                    渾身    None  \n",
       "1026                    映画    None  \n",
       "\n",
       "[1027 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#参考: https://qiita.com/junjis0203/items/fbde15dbba5ccdb5dc4c\n",
    "import pandas as pd\n",
    "\n",
    "def change_dict_key(d, old_key, new_key, default_value=None):\n",
    "    d[new_key] = d.pop(old_key, default_value)\n",
    "change_dict_key(cluster_to_words, 0, '日付、お店の評価、ネット用語に関するワード')\n",
    "change_dict_key(cluster_to_words, 1, '人や接客、内装に関するワード')\n",
    "change_dict_key(cluster_to_words, 2, 'その他のワード')\n",
    "change_dict_key(cluster_to_words, 3,  '券売機や注文に関するワード')\n",
    "change_dict_key(cluster_to_words, 4, '曜日時間、店舗の地理的なワード')\n",
    "change_dict_key(cluster_to_words, 5, 'ラーメンの中身に関するワード')\n",
    "\n",
    "df_dict = pd.DataFrame.from_dict(cluster_to_words, orient=\"index\").T\n",
    "df_dict.iloc[:,[5,3,1,4,0,2]] #ix->iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      日付、お店の評価、ネット用語に関するワード 人や接客、内装に関するワード その他のワード 券売機や注文に関するワード  \\\n",
       "0                     ラーメン         チャーシュー      さん           スープ   \n",
       "1                       こと           美味しい      訪問            醤油   \n",
       "2                       よう             これ      行列            旨味   \n",
       "3                     中華そば           ワンタン      到着            香り   \n",
       "4                      こちら             感じ      待ち            出汁   \n",
       "...                    ...            ...     ...           ...   \n",
       "1022                   づくり           None    None          None   \n",
       "1023                   50%           None    None          None   \n",
       "1024                 否が応でも           None    None          None   \n",
       "1025                    渾身           None    None          None   \n",
       "1026                    映画           None    None          None   \n",
       "\n",
       "     曜日時間、店舗の地理的なワード ラーメンの中身に関するワード  \n",
       "0               食べログ             そば  \n",
       "1               オープン             特製  \n",
       "2                 徒歩             店内  \n",
       "3                 名店             食券  \n",
       "4                 百名          カウンター  \n",
       "...              ...            ...  \n",
       "1022            None           None  \n",
       "1023            None           None  \n",
       "1024            None           None  \n",
       "1025            None           None  \n",
       "1026            None           None  \n",
       "\n",
       "[1027 rows x 6 columns]>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 https://qiita.com/tatsuya-miyamoto/items/f1539d86ad4980624111\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import pickle\n",
    "\n",
    "taste_words = cluster_to_words['ラーメンの中身に関するワード']\n",
    "kenbaiki_words = cluster_to_words['券売機や注文に関するワード']\n",
    "taste_words.extend(kenbaiki_words)\n",
    "ramen_word = taste_words\n",
    "cluster_to_words.keys()\n",
    "\n",
    "# 文書\n",
    "\n",
    "f = open('../work/ramen_corpus.txt','r',encoding=\"utf-8\")\n",
    "trainings = []\n",
    "\n",
    "for i,data in enumerate(f):\n",
    "    word = data.replace(\"'\",'').replace('[','').replace(']','').replace(' ','').replace('\\n','').split(\",\")\n",
    "    trainings.append([i for i in word if i in ramen_word])\n",
    "\n",
    "# 単語->id変換の辞書作成\n",
    "dictionary = corpora.Dictionary(trainings)\n",
    "\n",
    "# textsをcorpus化\n",
    "corpus = list(map(dictionary.doc2bow,trainings))\n",
    "\n",
    "# tfidf modelの生成\n",
    "test_model = models.TfidfModel(corpus)\n",
    "\n",
    "# corpusへのモデル適用\n",
    "corpus_tfidf = test_model[corpus]\n",
    "\n",
    "# id->単語へ変換\n",
    "texts_tfidf = [] # id -> 単語表示に変えた文書ごとのTF-IDF\n",
    "for doc in corpus_tfidf:\n",
    "    text_tfidf = []\n",
    "    for word in doc:\n",
    "        text_tfidf.append([dictionary[word[0]],word[1]])\n",
    "    texts_tfidf.append(text_tfidf)\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "texts_tfidf_sorted_top20 = [] \n",
    "\n",
    "#TF-IDF値を高い順に並び替え上位単語20個に絞る。\n",
    "for i in range(len(texts_tfidf)):\n",
    "    soted = sorted(texts_tfidf[i], key=itemgetter(1),reverse=True)\n",
    "    soted_top20 = soted[:20]\n",
    "    word_list = []\n",
    "    for k in range(len(soted_top20)):\n",
    "        word = soted_top20[k][0]\n",
    "        word_list.append(word)\n",
    "    texts_tfidf_sorted_top20.append(word_list)\n",
    "# 結果をデータフレームに追加\n",
    "\n",
    "df = pd.read_csv('../csv/tokyo_ramen_top20_100.csv')\n",
    "df_ramen = df.groupby(['store_name','score','review_cnt'])['review'].apply(list).apply(' '.join).reset_index().sort_values('score', ascending=False)\n",
    "df_ramen['texts_tfidf_sorted_top20'] = texts_tfidf_sorted_top20\n",
    "df_ramen['id'] = ['ID-' + str(i + 1).zfill(6) for i in range(len(df_ramen.index))]\n",
    "df_ramen_texts_tfidf_sorted_top20 = df_ramen.iloc[:,[5,3,1,4,0,2]].reset_index(drop=True)\n",
    "df_ramen_texts_tfidf_sorted_top20\n",
    "pickle.dump(df_ramen_texts_tfidf_sorted_top20, open('../work/df_ramen_texts_tfidf_sorted_top20', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ramen_texts_tfidf_sorted_top20.to_csv(\"df_ramen_texts_tfidf_sorted_top20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "f = open('../work/df_ramen_texts_tfidf_sorted_top20','rb')\n",
    "# f_pickle = f.to_pickle('alldata.pickle')\n",
    "store_df = pickle.load(f, encoding=\"utf-8\")\n",
    "store_cross = []\n",
    "for ids in product(store_df['id'], repeat=2):\n",
    "    store_cross.append(ids)\n",
    "\n",
    "store_cross_df = pd.DataFrame(store_cross, columns=['id_x', 'id_y'])\n",
    "\n",
    "store_cross_detail = store_cross_df.merge(\n",
    "    store_df[['id','store_name','score','review_cnt','texts_tfidf_sorted_top20']], how='inner', left_on='id_x', right_on='id'\n",
    ").drop(columns='id').merge(\n",
    "    store_df[['id','store_name','score','review_cnt','texts_tfidf_sorted_top20']], how='inner', left_on='id_y', right_on='id'\n",
    ").drop(columns='id')\n",
    "store_cross_detail = store_cross_detail[store_cross_detail['id_x'].isin(store_df['id'].loc[0:50])]\n",
    "store_cross_detail = store_cross_detail.reset_index(drop=True).sort_values(['id_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_cross_detail.to_csv(\"../csv/store_cross_detail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:01<00:00, 325.24it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-282dd5f03772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sim_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg_cos_sim_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mdf_sim_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'正規化'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mdf_sim_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-282dd5f03772>\u001b[0m in \u001b[0;36mmin_max\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmin_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.11/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     41\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     42\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "##ラーメン店xに対してラーメン店yの類似度を算出\n",
    "\n",
    "import itertools\n",
    "from tqdm import tqdm \n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "\n",
    "word2vec_ramen_model = word2vec.Word2Vec.load(\"../model/word2vec_ramen_model.model\")\n",
    "\n",
    "#コサイン類似度を算出する関数を定義\n",
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "#cossimだけの組み合わせ（同じワード同士の組みあわせがでてくるため）\n",
    "#2次元を１次元にする　setが重複を削除てきなやつ。\n",
    "uniq_words = list(set(itertools.chain.from_iterable(store_df['texts_tfidf_sorted_top20'].values)))\n",
    "scores = {}\n",
    "for word1, word2 in product(uniq_words, repeat=2):\n",
    "    scores[(word1, word2)] = cos_sim(word2vec_ramen_model.wv[word1],word2vec_ramen_model.wv[word2])\n",
    "\n",
    "\n",
    "avg_avg_scores = []\n",
    "for i in tqdm(range(len(store_cross_detail['texts_tfidf_sorted_top20_x']))):\n",
    "    avg_scores = []\n",
    "    for j in range(len(store_cross_detail['texts_tfidf_sorted_top20_x'][i])):\n",
    "        word_cross_scores = []\n",
    "        word_a = store_cross_detail['texts_tfidf_sorted_top20_x'][i][j]\n",
    "        for k in range(len(store_cross_detail['texts_tfidf_sorted_top20_y'][i])):\n",
    "            word_b = store_cross_detail['texts_tfidf_sorted_top20_y'][i][k]\n",
    "            score = scores[(word_a, word_b)]#単語間のスコアを出す。\n",
    "            word_cross_scores.append(score)\n",
    "        avg_scores.append(np.mean(word_cross_scores))#20個の単語間スコアの平均値\n",
    "    avg_avg_scores.append(np.mean(avg_scores))#20個の単語間スコアの平均値の平均値\n",
    "store_cross_detail.insert(6, 'avg_cos_sim_rate', avg_avg_scores)  \n",
    "# 「二郎」と類似度が高いラーメン屋を高い順に表示\n",
    "store_cross_detail = store_cross_detail.sort_values(['id_x', 'avg_cos_sim_rate'], ascending=[True, False])\n",
    "df_sim_x = store_cross_detail[store_cross_detail['store_name_x'].str.contains('二郎')]\n",
    "df_sim_x.reset_index(drop=True)\n",
    "\n",
    "def min_max(x, axis=None):\n",
    "    min = x.min(axis=axis, keepdims=True)\n",
    "    max = x.max(axis=axis, keepdims=True)\n",
    "    result = (x-min)/(max-min)\n",
    "    return result\n",
    "b = df_sim_x['avg_cos_sim_rate']\n",
    "c = min_max(b.values)\n",
    "df_sim_x.insert(7, '正規化', c)\n",
    "df_sim_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
