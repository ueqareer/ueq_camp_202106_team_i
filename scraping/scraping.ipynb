{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class Tabelog:\n",
    "    \"\"\"\n",
    "    食べログスクレイピングクラス\n",
    "    test_mode=Trueで動作させると、最初のページの３店舗のデータのみを取得できる\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, test_mode=False, p_ward='東京都内', begin_page=1, end_page=30):\n",
    "\n",
    "        # 変数宣言\n",
    "        self.store_id = ''\n",
    "        self.store_id_num = 0\n",
    "        self.store_name = ''\n",
    "        self.score = 0\n",
    "        self.ward = p_ward\n",
    "        self.review_cnt = 0\n",
    "        self.review = ''\n",
    "        self.columns = ['store_id', 'store_name', 'score', 'ward', 'review_cnt', 'review']\n",
    "        self.df = pd.DataFrame(columns=self.columns)\n",
    "        self.__regexcomp = re.compile(r'\\n|\\s') # \\nは改行、\\sは空白\n",
    "\n",
    "        page_num = begin_page # 店舗一覧ページ番号\n",
    "\n",
    "        if test_mode:\n",
    "            list_url = base_url + str(page_num) +  '/?Srt=D&SrtT=rt&sort_mode=1' #食べログの点数ランキングでソートする際に必要な処理\n",
    "            self.scrape_list(list_url, mode=test_mode)\n",
    "        else:\n",
    "            while True:\n",
    "                list_url = base_url + str(page_num) +  '/?Srt=D&SrtT=rt&sort_mode=1' #食べログの点数ランキングでソートする際に必要な処理\n",
    "                if self.scrape_list(list_url, mode=test_mode) != True:\n",
    "                    break\n",
    "\n",
    "                # INパラメータまでのページ数データを取得する\n",
    "                if page_num >= end_page:\n",
    "                    break\n",
    "                page_num += 1\n",
    "        return\n",
    "\n",
    "    def scrape_list(self, list_url, mode):\n",
    "        \"\"\"\n",
    "        店舗一覧ページのパーシング\n",
    "        \"\"\"\n",
    "        r = requests.get(list_url)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            return False\n",
    "\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        soup_a_list = soup.find_all('a', class_='list-rst__rst-name-target') # 店名一覧\n",
    "\n",
    "        if len(soup_a_list) == 0:\n",
    "            return False\n",
    "\n",
    "        if mode:\n",
    "            for soup_a in soup_a_list[:2]:\n",
    "                item_url = soup_a.get('href') # 店の個別ページURLを取得\n",
    "                self.store_id_num += 1\n",
    "                self.scrape_item(item_url, mode)\n",
    "        else:\n",
    "            for soup_a in soup_a_list:\n",
    "                item_url = soup_a.get('href') # 店の個別ページURLを取得\n",
    "                self.store_id_num += 1\n",
    "                self.scrape_item(item_url, mode)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def scrape_item(self, item_url, mode):\n",
    "        \"\"\"\n",
    "        個別店舗情報ページのパーシング\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        r = requests.get(item_url)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(f'error:not found{ item_url }')\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        # 店舗名称取得\n",
    "        # <h2 class=\"display-name\">\n",
    "        #     <span>\n",
    "        #         麺匠　竹虎 新宿店\n",
    "        #     </span>\n",
    "        # </h2>\n",
    "        store_name_tag = soup.find('h2', class_='display-name')\n",
    "        store_name = store_name_tag.span.string\n",
    "        print('{}→店名：{}'.format(self.store_id_num, store_name.strip()), end='')\n",
    "        self.store_name = store_name.strip()\n",
    "\n",
    "        # ラーメン屋、つけ麺屋以外の店舗は除外\n",
    "        store_head = soup.find('div', class_='rdheader-subinfo') # 店舗情報のヘッダー枠データ取得\n",
    "        store_head_list = store_head.find_all('dl')\n",
    "        store_head_list = store_head_list[1].find_all('span')\n",
    "        #print('ターゲット：', store_head_list[0].text)\n",
    "\n",
    "        if store_head_list[0].text not in {'ラーメン', 'つけ麺'}:\n",
    "            print('ラーメンorつけ麺のお店ではないので処理対象外')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "\n",
    "        # 評価点数取得\n",
    "        #<b class=\"c-rating__val rdheader-rating__score-val\" rel=\"v:rating\">\n",
    "        #    <span class=\"rdheader-rating__score-val-dtl\">3.58</span>\n",
    "        #</b>\n",
    "        rating_score_tag = soup.find('b', class_='c-rating__val')\n",
    "        rating_score = rating_score_tag.span.string\n",
    "        print('  評価点数：{}点'.format(rating_score), end='')\n",
    "        self.score = rating_score\n",
    "\n",
    "        # 評価点数が存在しない店舗は除外\n",
    "        if rating_score == '-':\n",
    "            print('  評価がないため処理対象外')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "       # 評価が3.5未満店舗は除外\n",
    "        if float(rating_score) < 3.5:\n",
    "            print('  食べログ評価が3.5未満のため処理対象外')\n",
    "            self.store_id_num -= 1\n",
    "            return\n",
    "\n",
    "        # レビュー一覧URL取得\n",
    "        #<a class=\"mainnavi\" href=\"https://tabelog.com/tokyo/A1304/A130401/13143442/dtlrvwlst/\"><span>口コミ</span><span class=\"rstdtl-navi__total-count\"><em>60</em></span></a>\n",
    "        review_tag_id = soup.find('li', id=\"rdnavi-review\")\n",
    "        review_tag = review_tag_id.a.get('href')\n",
    "\n",
    "        # レビュー件数取得\n",
    "        print('  レビュー件数：{}'.format(review_tag_id.find('span', class_='rstdtl-navi__total-count').em.string), end='')\n",
    "        self.review_cnt = review_tag_id.find('span', class_='rstdtl-navi__total-count').em.string\n",
    "\n",
    "        # レビュー一覧ページ番号\n",
    "        page_num = 1 #1ページ*20 = 20レビュー 。この数字を変えて取得するレビュー数を調整。\n",
    "\n",
    "        # レビュー一覧ページから個別レビューページを読み込み、パーシング\n",
    "        # 店舗の全レビューを取得すると、食べログの評価ごとにデータ件数の濃淡が発生してしまうため、\n",
    "        # 取得するレビュー数は１ページ分としている（件数としては１ページ*20=２0レビュー）\n",
    "        while True:\n",
    "            review_url = review_tag + 'COND-0/smp1/?lc=0&rvw_part=all&PG=' + str(page_num)\n",
    "            #print('\\t口コミ一覧リンク：{}'.format(review_url))\n",
    "            print(' . ' , end='') #LOG\n",
    "            if self.scrape_review(review_url) != True:\n",
    "                break\n",
    "            if page_num >= 5:\n",
    "                break\n",
    "            page_num += 1\n",
    "\n",
    "        process_time = time.time() - start\n",
    "        print('  取得時間：{}'.format(process_time))\n",
    "\n",
    "        return\n",
    "\n",
    "    def scrape_review(self, review_url):\n",
    "        \"\"\"\n",
    "        レビュー一覧ページのパーシング\n",
    "        \"\"\"\n",
    "        r = requests.get(review_url)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(f'error:not found{ review_url }')\n",
    "            return False\n",
    "\n",
    "        # 各個人の口コミページ詳細へのリンクを取得する\n",
    "        #<div class=\"rvw-item js-rvw-item-clickable-area\" data-detail-url=\"/tokyo/A1304/A130401/13141542/dtlrvwlst/B408082636/?use_type=0&amp;smp=1\">\n",
    "        #</div>\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        review_url_list = soup.find_all('div', class_='rvw-item') # 口コミ詳細ページURL一覧\n",
    "\n",
    "        if len(review_url_list) == 0:\n",
    "            return False\n",
    "\n",
    "        for url in review_url_list:\n",
    "            review_detail_url = 'https://tabelog.com' + url.get('data-detail-url')\n",
    "            #print('\\t口コミURL：', review_detail_url)\n",
    "\n",
    "            # 口コミのテキストを取得\n",
    "            self.get_review_text(review_detail_url)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_review_text(self, review_detail_url):\n",
    "        \"\"\"\n",
    "        口コミ詳細ページをパーシング\n",
    "        \"\"\"\n",
    "        r = requests.get(review_detail_url)\n",
    "        if r.status_code != requests.codes.ok:\n",
    "            print(f'error:not found{ review_detail_url }')\n",
    "            return\n",
    "\n",
    "        # ２回以上来訪してコメントしているユーザは最新の1件のみを採用\n",
    "        #<div class=\"rvw-item__rvw-comment\" property=\"v:description\">\n",
    "        #  <p>\n",
    "        #    <br>すごい煮干しラーメン凪 新宿ゴールデン街本館<br>スーパーゴールデン1600円（20食限定）を喰らう<br>大盛り無料です<br>スーパーゴールデンは、新宿ゴールデン街にちなんで、ココ本店だけの特別メニューだそうです<br>相方と歌舞伎町のtohoシネマズの映画館でドラゴンボール超ブロリー を観てきた<br>ブロリー 強すぎるね(^^)面白かったです<br>凪の煮干しラーメンも激ウマ<br>いったん麺ちゅるちゅる感に、レアチャーと大トロチャーシューのトロけ具合もうめえ<br>煮干しスープもさすが！と言うほど完成度が高い<br>さすが食べログラーメン百名店<br>と言うか<br>2日連チャンで、近場の食べログラーメン百名店のうちの2店舗、昨日の中華そば葉山さんと今日の凪<br>静岡では考えられん笑笑<br>ごちそうさまでした\n",
    "        #  </p>\n",
    "        #</div>\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        review = soup.find_all('div', class_='rvw-item__rvw-comment')#reviewが含まれているタグの中身をすべて取得\n",
    "        if len(review) == 0:\n",
    "            review = ''\n",
    "        else:\n",
    "            review = review[0].p.text.strip() # strip()は改行コードを除外する関数\n",
    "\n",
    "        #print('\\t\\t口コミテキスト：', review)\n",
    "        self.review = review\n",
    "\n",
    "        # データフレームの生成\n",
    "        self.make_df()\n",
    "        return\n",
    "\n",
    "    def make_df(self):\n",
    "        self.store_id = str(self.store_id_num).zfill(8) #0パディング\n",
    "        se = pd.Series([self.store_id, self.store_name, self.score, self.ward, self.review_cnt, self.review], self.columns) # 行を作成\n",
    "        self.df = self.df.append(se, self.columns) # データフレームに行を追加\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1→店名：らぁ麺や 嶋  評価点数：4.10点  レビュー件数：213 .  .  .  .  .   取得時間：133.79664516448975\n",
      "2→店名：手打式超多加水麺 ののくら  評価点数：3.98点  レビュー件数：496 .  .  .  .  .   取得時間：164.98543000221252\n",
      "3→店名：中華そば しば田  評価点数：3.95点  レビュー件数：836 .  .  .  .  .   取得時間：221.65730094909668\n",
      "4→店名：麺尊 RAGE  評価点数：3.95点  レビュー件数：755 .  .  .  .  .   取得時間：213.08797311782837\n",
      "5→店名：宍道湖しじみ中華蕎麦 琥珀  評価点数：3.95点  レビュー件数：326 .  .  .  .  .   取得時間：140.84992790222168\n",
      "6→店名：らぁ麺やまぐち  評価点数：3.94点  レビュー件数：454 .  .  .  .  .   取得時間：190.72020602226257\n",
      "7→店名：麺処 しろくろ  評価点数：3.94点  レビュー件数：167 .  .  .  .  .   取得時間：126.7122278213501\n",
      "8→店名：中華蕎麦にし乃  評価点数：3.93点  レビュー件数：649 .  .  .  .  .   取得時間：188.38033294677734\n",
      "9→店名：メンドコロ キナリ  評価点数：3.93点  レビュー件数：401 .  .  .  .  .   取得時間：163.44144415855408\n",
      "10→店名：創作麺工房 鳴龍ラーメンorつけ麺のお店ではないので処理対象外\n",
      "10→店名：麺処 ほん田 秋葉原本店  評価点数：3.92点  レビュー件数：405 .  .  .  .  .   取得時間：159.50888109207153\n",
      "11→店名：キング製麺  評価点数：3.92点  レビュー件数：303 .  .  .  .  .   取得時間：142.64221692085266\n",
      "12→店名：中華そば うお青  評価点数：3.92点  レビュー件数：132 .  .  .  .  .   取得時間：122.49715209007263\n",
      "13→店名：饗 くろ喜  評価点数：3.91点  レビュー件数：1077 .  .  .  .  .   取得時間：280.03131771087646\n",
      "14→店名：中華そば 多賀野  評価点数：3.91点  レビュー件数：836 .  .  .  .  .   取得時間：230.25643801689148\n",
      "15→店名：Homemade Ramen 麦苗  評価点数：3.91点  レビュー件数：744 .  .  .  .  .   取得時間：225.78412103652954\n",
      "16→店名：寿製麺 よしかわ 西台駅前店  評価点数：3.91点  レビュー件数：258 .  .  .  .  .   取得時間：140.79456686973572\n",
      "17→店名：一番いちばん  評価点数：3.90点  レビュー件数：326 .  .  .  .  .   取得時間：149.45191287994385\n",
      "18→店名：らーめんMAIKAGURA  評価点数：3.90点  レビュー件数：304 .  .  .  .  .   取得時間：143.35747408866882\n",
      "19→店名：純手打ち だるま  評価点数：3.90点  レビュー件数：277 .  .  .  .  .   取得時間：141.53358912467957\n"
     ]
    }
   ],
   "source": [
    "tokyo_ramen_review = Tabelog(base_url=\"https://tabelog.com/tokyo/rstLst/ramen/\",test_mode=False, p_ward='東京都内',end_page=1)\n",
    "#CSV保存\n",
    "tokyo_ramen_review.df.to_csv(\"./tokyo_ramen_top20_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
